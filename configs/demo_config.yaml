# AI Recruitment Assistant - Demo Configuration
# ==============================================
# Quick demo config for fast training and testing

# Model Configuration
model:
  name: "microsoft/DialoGPT-small"  # Much smaller model for demo
  max_length: 256
  cache_dir: "./models/cache"

# LoRA Configuration (Lighter for demo)
lora:
  r: 8  # Smaller rank
  alpha: 16
  dropout: 0.1
  target_modules:
    - "c_attn"  # DialoGPT specific
  bias: "none"
  task_type: "CAUSAL_LM"

# Quantization Settings
quantization:
  use_4bit: false  # Disable for demo
  use_nested_quant: false
  compute_dtype: "float32"
  quant_type: "nf4"

# Training Parameters (Fast demo)
training:
  num_epochs: 1  # Just 1 epoch for demo
  batch_size: 2
  gradient_accumulation_steps: 2
  learning_rate: 0.0005  # Higher LR for faster convergence
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  
  # Optimizer settings
  optim: "adamw_torch"  # Standard optimizer
  lr_scheduler_type: "linear"
  
  # Memory optimization
  gradient_checkpointing: false  # Disable for speed
  dataloader_drop_last: true
  group_by_length: false
  
  # Mixed precision
  fp16: false
  bf16: false

# Data Configuration
data:
  dataset_path: "data/training/alpaca_format.json"
  max_examples: 50  # Only use first 50 examples for demo
  train_split: 1.0  # Use all for training in demo
  val_split: 0.0
  seed: 42

# Paths
paths:
  output_dir: "models/demo-checkpoints"
  final_model_dir: "models/demo-fine-tuned"
  logs_dir: "logs"
  data_dir: "data"

# Logging (More frequent for demo)
logging:
  level: "INFO"
  log_steps: 5
  eval_steps: 25
  save_steps: 25
  save_total_limit: 1

# Weights & Biases (Disabled for demo)
wandb:
  project: "ai-recruitment-assistant-demo"
  entity: null
  run_name: "demo-quick-train"
  tags:
    - "demo"
    - "quick-test"
  enabled: false  # Disable for demo

# Hardware Optimization
hardware:
  device: "auto"
  mixed_precision: false
  compile_model: false

# Generation Settings
generation:
  max_new_tokens: 128
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true
  repetition_penalty: 1.1

# Evaluation
evaluation:
  eval_strategy: "no"  # No evaluation for demo
  load_best_model_at_end: false
